projectPath = /users/cuffe002/desktop/projects/mamba/mamba2/demo_business_data ##the path for our output files
data1_name = data1_test  ###First dataset name
data2_name = data2_test  ###Second dataset name.
sql_flavor = sqlite ###what flavor of sql are you using? current options: sqlite or postgres.  if using postgres, add in db_user, db_port, db_password, and db_host as entries
db_name = mamba_filter_db ###database name
debugmode = True ###run in debug mode?
block_file_name = block_names.csv ###The file (including .csv ending) where the names of the blocking variables live
create_db_chunksize = 100000  ###the size of the chunks we want to read in
training_data_name = training_data  ###name of the training data.csv file.  don't include .csv ending
rf_jobs = 1  ###Number of jobs in the random forest
clerical_review_candidates = True  ##do we want to generate clerical review candidates
###Two values: variable: the name of the variable from your mamba_variable_types that you want to filter clerical review candidates on
###NOTE: if you're using a fuzzy variable, have it be the pattern {variable name}_{fuzzy comparator}.  So "name_jaro" to look at the jaro score of the name.
###If you are using the predicted probability from a model, make this "predicted_probability"
###value: in double quotes, the lower value you want to consider.
clerical_review_threshold = {"variable":"predicted_probability", "value":".3"}
match_threshold = .5  ##what is the match threshold you want to look at
chatty_logger = True  ##if on, logger logs after every block
log_file_name = mamba_test_log ##the anme of your log file
numWorkers = 3 ##number of workers in the Runner object
prediction = True ##are we generating match predictions?
scoringcriteria = accuracy
ignore_duplicate_ids = False ###are we trying to depulicate the same dataset? This assumes the IDs are the same on each set (so a record in A has the same record in B)
use_logit = False ####Use the logit.  If you do this, scoring will be converted to accuracy (only built-in feature)
date_format = %Y-%m-%d   ###format for any dates
stem_phrase = False ###Use the stemming/phrasing feature?
parse_address = True   ###are we using the address parsing?
address_column_data1_test = address_unparsed   ###name of address column needed parsed in the first dataset
address_column_data2_test = address_raw ###name of address column needed parsed in the second dataset
use_remaining_parsed_address = False ##are we using thee leftover address parsing info to match?
standardize_addresses = False ###do you want to standarize addresses?
####if for a dataset you don't want to standardize for a particular dataset, don't include a json entry
address_to_standardize = {}
###Are we running a custom model?
use_custom_model = False
###Are we using the build in mamba models?
use_mamba_models = True
###What imputation model are we using: Imputation = impute values using an iterative imputer,Nnominal means cut fuzzy vars
imputation_method = Nominal
##Are you using a previously used model? include the '.joblib' file suffix. enter 'False' otherwise
saved_model = False
##If you have a new model, do you want to save it? If so, enter the filename (including the .joblib ending)
###Note.  If use_saved_model has a .joblib ending, this will be ignored.
saved_model_target = newmod.joblib
###Do you want to run in recursive feature elimination mode?
feature_elimination_mode = True
###Do you want to run a pre-filter for your models?
use_variable_filter = True
###If use_variable_filter is True, details here.  See readme for available options
variable_filter_info = {"variable_name": "name", "fuzzy_name": "jaro", "test":">", "filter_value":".65"}
